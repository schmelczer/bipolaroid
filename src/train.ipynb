{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Using device cuda:0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "f\"Using device {device}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform, uniform\n",
    "\n",
    "\n",
    "common_hyperparameters = {\n",
    "    \"batch_size\": [8, 16, 32, 64],\n",
    "    \"edit_count\": [8, 16, 32],\n",
    "    \"bin_count\": [16, 32, 64],\n",
    "    \"clip_gradients\": [True, False],\n",
    "    \"learning_rate\": loguniform(0.00001, 0.005),\n",
    "    \"scheduler_gamma\": uniform(0.1, 0.9),\n",
    "    \"num_epochs\": [10],\n",
    "    \"model_type\": [\n",
    "        \"NormalisedCNN\",\n",
    "        \"SimpleCNN\",\n",
    "        \"Residual\",\n",
    "        \"SmartRes\",\n",
    "        \"Res2\",\n",
    "    ],\n",
    "}\n",
    "hyperparameters = [\n",
    "    # {\n",
    "    #     **common_hyperparameters,\n",
    "    #     \"loss\": [\"progressive\"],\n",
    "    #     \"loss_sizes\": [[4, 8, 16, 32], [8, 16, 32], [16, 32], [8, 32]],\n",
    "    #     \"loss_damping\": uniform(0.2, 5),\n",
    "    # },\n",
    "    {\n",
    "        **common_hyperparameters,\n",
    "        \"loss\": [\"kl\"],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "from torch.optim import Adam\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from editor.training import ProgressivePoolingLoss\n",
    "from editor.utils import get_next_run_name\n",
    "from editor.visualisation import plot_histograms_in_2d\n",
    "from editor.training import create_data_loaders\n",
    "from editor.models import create_model\n",
    "from config import DATA, MODELS_PATH\n",
    "\n",
    "\n",
    "def train(hyperparameters: Dict[str, Any]) -> Path:\n",
    "    model_path = (MODELS_PATH / get_next_run_name(Path(\"runs\"))).with_suffix(\".pth\")\n",
    "\n",
    "    log_dir = Path(\"runs\") / get_next_run_name(Path(\"runs\"))\n",
    "    with SummaryWriter(log_dir) as writer:\n",
    "        train_data_loader, test_data_loader = create_data_loaders(\n",
    "            data=DATA,\n",
    "            edit_count=hyperparameters[\"edit_count\"],\n",
    "            bin_count=hyperparameters[\"bin_count\"],\n",
    "            training_batch_size=hyperparameters[\"batch_size\"],\n",
    "        )\n",
    "\n",
    "        model = (\n",
    "            create_model(\n",
    "                type=hyperparameters[\"model_type\"],\n",
    "                bin_count=hyperparameters[\"bin_count\"],\n",
    "            )\n",
    "            .train()\n",
    "            .to(device)\n",
    "        )\n",
    "        writer.add_graph(model, next(iter(train_data_loader))[0].to(device))\n",
    "\n",
    "        optimizer = Adam(model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer, step_size=1, gamma=hyperparameters[\"scheduler_gamma\"]\n",
    "        )\n",
    "\n",
    "        loss_function = {\n",
    "            \"progressive\": lambda: ProgressivePoolingLoss(\n",
    "                target_sizes=hyperparameters[\"loss_sizes\"],\n",
    "                damping=hyperparameters[\"loss_damping\"],\n",
    "            ),\n",
    "            \"kl\": lambda: torch.nn.KLDivLoss(reduction=\"batchmean\"),\n",
    "        }[hyperparameters[\"loss\"]]().to(device)\n",
    "\n",
    "        try:\n",
    "            for epoch in range(hyperparameters[\"num_epochs\"]):\n",
    "                epoch_loss = 0\n",
    "                writer.add_scalar(\n",
    "                    \"Actual learning rate\", scheduler.get_last_lr()[0], epoch\n",
    "                )\n",
    "                for batch_id, (edited_histogram, original_histogram) in enumerate(\n",
    "                    tqdm(train_data_loader, desc=f\"Epoch {epoch}\", unit=\"batch\")\n",
    "                ):\n",
    "                    edited_histogram = edited_histogram.to(device)\n",
    "                    original_histogram = original_histogram.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    predicted_original = model(edited_histogram)\n",
    "                    sum = torch.sum(predicted_original, dim=(2, 3, 4), keepdim=True)\n",
    "                    predicted_original = predicted_original / sum\n",
    "\n",
    "                    if hyperparameters[\"loss\"] == \"kl\":\n",
    "                        predicted_original = torch.clamp(\n",
    "                            predicted_original, 0.0000000000000000000001, 1\n",
    "                        )\n",
    "\n",
    "                    loss = {\n",
    "                        \"kl\": lambda: loss_function(\n",
    "                            torch.log(predicted_original),\n",
    "                            original_histogram,\n",
    "                        ),\n",
    "                        \"progressive\": lambda: loss_function(\n",
    "                            predicted_original, original_histogram\n",
    "                        ),\n",
    "                    }[hyperparameters[\"loss\"]]()\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "                    writer.add_scalar(\n",
    "                        \"Loss/train/batch\",\n",
    "                        loss,\n",
    "                        epoch * len(train_data_loader) + batch_id,\n",
    "                    )\n",
    "                    loss.backward()\n",
    "\n",
    "                    if hyperparameters[\"clip_gradients\"]:\n",
    "                        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "\n",
    "                writer.add_hparams(\n",
    "                    {\n",
    "                        k: str(v) if isinstance(v, list) else v\n",
    "                        for k, v in hyperparameters.items()\n",
    "                    },\n",
    "                    {\n",
    "                        \"Loss/train/epoch\": epoch_loss,\n",
    "                    },\n",
    "                    global_step=epoch,\n",
    "                    run_name=log_dir.absolute(),\n",
    "                )\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    loader = iter(test_data_loader)\n",
    "                    edited_histogram, original_histogram = next(loader)\n",
    "                    edited_histogram = edited_histogram.to(device)\n",
    "                    original_histogram = original_histogram.to(device)\n",
    "                    predicted_original = model(edited_histogram)\n",
    "                    sum = torch.sum(predicted_original, dim=(2, 3, 4), keepdim=True)\n",
    "                    predicted_original = predicted_original / sum\n",
    "                    writer.add_figure(\n",
    "                        \"histogram\",\n",
    "                        plot_histograms_in_2d(\n",
    "                            {\n",
    "                                \"original\": original_histogram.cpu()[0]\n",
    "                                .numpy()\n",
    "                                .squeeze(),\n",
    "                                \"edited\": edited_histogram.cpu()[0].numpy().squeeze(),\n",
    "                                \"predicted\": predicted_original.cpu()[0]\n",
    "                                .numpy()\n",
    "                                .squeeze(),\n",
    "                            }\n",
    "                        ),\n",
    "                        epoch,\n",
    "                    )\n",
    "                    model.train()\n",
    "                scheduler.step()\n",
    "        except Exception as e:\n",
    "            raise\n",
    "        finally:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "        return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(\n",
    "#     {\n",
    "#         \"batch_size\": 64,\n",
    "#         \"edit_count\": 25,\n",
    "#         \"bin_count\": 32,\n",
    "#         \"clip_gradients\": True,\n",
    "#         \"learning_rate\": 0.005,\n",
    "#         \"scheduler_gamma\": 0.7,\n",
    "#         \"num_epochs\": 20,\n",
    "#         \"model_type\": \"NormalisedCNN\",\n",
    "#         \"loss\": \"progressive\",\n",
    "#         \"loss_sizes\": [16, 32],\n",
    "#         \"loss_damping\": 2,\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 21:54:49,789 - INFO - Starting run_71 with hparams {'batch_size': 8, 'edit_count': 8, 'bin_count': 32, 'clip_gradients': True, 'learning_rate': 0.0001, 'scheduler_gamma': 0.5, 'num_epochs': 2, 'model_type': 'Res2', 'loss': 'kl'}\n",
      "2024-05-12 21:54:49,792 - INFO - Loaded 72 training images and 8 test images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461659956f3944a085b5cc3a5af6ec31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/9 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andras/miniconda3/envs/bipolaroid/lib/python3.12/site-packages/matplotlib/collections.py:996: RuntimeWarning: invalid value encountered in sqrt\n",
      "  scale = np.sqrt(self._sizes) * dpi / 72.0 * self._factor\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea8edb16ad742d0ab6673560f231b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/9 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 21:55:07,098 - INFO - Starting run_72 with hparams {'batch_size': 8, 'edit_count': 8, 'bin_count': 32, 'clip_gradients': True, 'learning_rate': 0.0001, 'scheduler_gamma': 0.5, 'num_epochs': 2, 'model_type': 'Residual', 'loss': 'kl'}\n",
      "2024-05-12 21:55:07,100 - INFO - Loaded 72 training images and 8 test images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede7a736204f4198868a98381b8862a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/9 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707b28e45e3448fc9e449624c9a8467e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/9 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 21:55:21,888 - INFO - Starting run_73 with hparams {'batch_size': 8, 'edit_count': 8, 'bin_count': 32, 'clip_gradients': True, 'learning_rate': 0.0001, 'scheduler_gamma': 0.5, 'num_epochs': 2, 'model_type': 'SimpleCNN', 'loss': 'kl'}\n",
      "2024-05-12 21:55:21,890 - INFO - Loaded 72 training images and 8 test images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f94f38fd9c245bd9cc99ffe9cb0c058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/9 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bdbfb83837640099d2c1b5e8ac0cfe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/9 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 21:55:36,871 - INFO - Starting run_74 with hparams {'batch_size': 8, 'edit_count': 8, 'bin_count': 32, 'clip_gradients': True, 'learning_rate': 0.0001, 'scheduler_gamma': 0.5, 'num_epochs': 2, 'model_type': 'NormalisedCNN', 'loss': 'kl'}\n",
      "2024-05-12 21:55:36,872 - INFO - Loaded 72 training images and 8 test images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b415e0ac9334aaf89b0fd8e234aa7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/9 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e632fae2e4b4929b6e70dffcad6a341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/9 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 21:55:52,083 - INFO - Starting run_75 with hparams {'batch_size': 8, 'edit_count': 8, 'bin_count': 32, 'clip_gradients': True, 'learning_rate': 0.0001, 'scheduler_gamma': 0.5, 'num_epochs': 2, 'model_type': 'SmartRes', 'loss': 'kl'}\n",
      "2024-05-12 21:55:52,084 - INFO - Loaded 72 training images and 8 test images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46909535e98543ccb700caeb718dde48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/9 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f757acc512484e9ac0f34efcd4c1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/9 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from random import choice\n",
    "from itertools import count\n",
    "import json\n",
    "\n",
    "\n",
    "tried = set()\n",
    "\n",
    "for _ in count():\n",
    "    current_hyperparameters = {\n",
    "        k: v.rvs() if hasattr(v, \"rvs\") else choice(v)\n",
    "        for k, v in choice(hyperparameters).items()\n",
    "    }\n",
    "    key = json.dumps(current_hyperparameters)\n",
    "    if key in tried:\n",
    "        continue\n",
    "    tried.add(key)\n",
    "    logging.info(\n",
    "        f\"Starting {get_next_run_name(Path(\"runs\"))} with hparams {current_hyperparameters}\"\n",
    "    )\n",
    "    try:\n",
    "        train(current_hyperparameters)\n",
    "    except KeyboardInterrupt as e:\n",
    "        logging.info(\"Interrupted, stopping\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        logging.error(\n",
    "            f\"Error with hparams {current_hyperparameters}:\\n\\t{e}\", stack_info=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: try different colour spaces, see the results applied to images\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bipolaroid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
