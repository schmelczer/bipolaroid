{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Using device cuda:0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(f\"train-{datetime.now().date()}.log\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "f\"Using device {device}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform, uniform, randint\n",
    "from editor.models import MODELS\n",
    "\n",
    "common_hyperparameters = {\n",
    "    \"batch_size\": [16, 32, 64],\n",
    "    \"edit_count\": [8, 16],\n",
    "    \"bin_count\": [16, 24, 32],\n",
    "    \"clip_gradients\": [True, False],\n",
    "    \"learning_rate\": loguniform(0.00001, 0.01),\n",
    "    \"scheduler_gamma\": uniform(0, 1),\n",
    "    \"num_epochs\": randint(5, 10),\n",
    "    \"model_type\": list(MODELS.keys()),\n",
    "}\n",
    "hyperparameters = [\n",
    "    # {\n",
    "    #     **common_hyperparameters,\n",
    "    #     \"loss\": [\"progressive\"],\n",
    "    #     \"loss_sizes\": [[4, 8, 16, 32], [8, 16, 32], [16, 32], [8, 32]],\n",
    "    #     \"loss_damping\": uniform(0.2, 5),\n",
    "    # },\n",
    "    {\n",
    "        **common_hyperparameters,\n",
    "        \"loss\": [\"kl\"],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "from torch.optim import Adam\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from editor.training import ProgressivePoolingLoss\n",
    "from editor.utils import get_next_run_name\n",
    "from editor.visualisation import plot_histograms_in_2d\n",
    "from editor.training import create_data_loaders\n",
    "from editor.models import create_model, test_models\n",
    "from config import DATA, MODELS_PATH\n",
    "from datetime import timedelta, datetime\n",
    "import json\n",
    "\n",
    "\n",
    "# test_models()\n",
    "\n",
    "\n",
    "def train(\n",
    "    hyperparameters: Dict[str, Any], max_duration: timedelta, use_tqdm: bool\n",
    ") -> Path:\n",
    "    start_time = datetime.now()\n",
    "    model_path = (MODELS_PATH / get_next_run_name(Path(\"runs\"))).with_suffix(\".pth\")\n",
    "    params_path = (MODELS_PATH / get_next_run_name(Path(\"runs\"))).with_suffix(\".json\")\n",
    "    with open(params_path, \"w\") as f:\n",
    "        json.dump(hyperparameters, f, indent=2)\n",
    "\n",
    "    log_dir = Path(\"runs\") / get_next_run_name(Path(\"runs\"))\n",
    "    with SummaryWriter(log_dir) as writer:\n",
    "        train_data_loader, test_data_loader = create_data_loaders(\n",
    "            data=DATA,\n",
    "            edit_count=hyperparameters[\"edit_count\"],\n",
    "            bin_count=hyperparameters[\"bin_count\"],\n",
    "            training_batch_size=hyperparameters[\"batch_size\"],\n",
    "        )\n",
    "\n",
    "        model = (\n",
    "            create_model(\n",
    "                type=hyperparameters[\"model_type\"],\n",
    "                bin_count=hyperparameters[\"bin_count\"],\n",
    "            )\n",
    "            .train()\n",
    "            .to(device)\n",
    "        )\n",
    "        writer.add_graph(model, next(iter(train_data_loader))[0].to(device))\n",
    "\n",
    "        optimizer = Adam(model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer, step_size=1, gamma=hyperparameters[\"scheduler_gamma\"]\n",
    "        )\n",
    "\n",
    "        loss_function = {\n",
    "            \"progressive\": lambda: ProgressivePoolingLoss(\n",
    "                target_sizes=hyperparameters[\"loss_sizes\"],\n",
    "                damping=hyperparameters[\"loss_damping\"],\n",
    "            ),\n",
    "            \"kl\": lambda: torch.nn.KLDivLoss(reduction=\"batchmean\"),\n",
    "        }[hyperparameters[\"loss\"]]().to(device)\n",
    "\n",
    "        try:\n",
    "            for epoch in range(hyperparameters[\"num_epochs\"]):\n",
    "                epoch_loss = 0\n",
    "                writer.add_scalar(\n",
    "                    \"Actual learning rate\", scheduler.get_last_lr()[0], epoch\n",
    "                )\n",
    "                for batch_id, (edited_histogram, original_histogram) in enumerate(\n",
    "                    tqdm(train_data_loader, desc=f\"Epoch {epoch}\", unit=\"batch\")\n",
    "                    if use_tqdm\n",
    "                    else train_data_loader\n",
    "                ):\n",
    "                    current_time = datetime.now()\n",
    "                    if current_time - start_time > max_duration:\n",
    "                        raise TimeoutError(f\"Time limit {max_duration} exceeded\")\n",
    "                    edited_histogram = edited_histogram.to(device)\n",
    "                    original_histogram = original_histogram.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    predicted_original = model(edited_histogram)\n",
    "                    sum = torch.sum(predicted_original, dim=(2, 3, 4), keepdim=True)\n",
    "                    predicted_original = predicted_original / sum\n",
    "\n",
    "                    if hyperparameters[\"loss\"] == \"kl\":\n",
    "                        predicted_original = torch.clamp(\n",
    "                            predicted_original, 0.0000000000000000000001, 1\n",
    "                        )\n",
    "\n",
    "                    loss = {\n",
    "                        \"kl\": lambda: loss_function(\n",
    "                            torch.log(predicted_original),\n",
    "                            original_histogram,\n",
    "                        ),\n",
    "                        \"progressive\": lambda: loss_function(\n",
    "                            predicted_original, original_histogram\n",
    "                        ),\n",
    "                    }[hyperparameters[\"loss\"]]()\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "                    writer.add_scalar(\n",
    "                        \"Loss/train/batch\",\n",
    "                        loss,\n",
    "                        epoch * len(train_data_loader) + batch_id,\n",
    "                    )\n",
    "                    loss.backward()\n",
    "\n",
    "                    if hyperparameters[\"clip_gradients\"]:\n",
    "                        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "\n",
    "                writer.add_hparams(\n",
    "                    {\n",
    "                        k: str(v) if isinstance(v, list) else v\n",
    "                        for k, v in hyperparameters.items()\n",
    "                    },\n",
    "                    {\n",
    "                        \"Loss/train/epoch\": epoch_loss,\n",
    "                    },\n",
    "                    global_step=epoch,\n",
    "                    run_name=log_dir.absolute(),\n",
    "                )\n",
    "                logging.info(f\"Epoch {epoch} loss: {epoch_loss}\")\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    loader = iter(test_data_loader)\n",
    "                    edited_histogram, original_histogram = next(loader)\n",
    "                    edited_histogram = edited_histogram.to(device)\n",
    "                    original_histogram = original_histogram.to(device)\n",
    "                    predicted_original = model(edited_histogram)\n",
    "                    sum = torch.sum(predicted_original, dim=(2, 3, 4), keepdim=True)\n",
    "                    predicted_original = predicted_original / sum\n",
    "                    writer.add_figure(\n",
    "                        \"histogram\",\n",
    "                        plot_histograms_in_2d(\n",
    "                            {\n",
    "                                \"original\": original_histogram.cpu()[0]\n",
    "                                .numpy()\n",
    "                                .squeeze(),\n",
    "                                \"edited\": edited_histogram.cpu()[0].numpy().squeeze(),\n",
    "                                \"predicted\": predicted_original.cpu()[0]\n",
    "                                .numpy()\n",
    "                                .squeeze(),\n",
    "                            }\n",
    "                        ),\n",
    "                        epoch,\n",
    "                    )\n",
    "                    model.train()\n",
    "                scheduler.step()\n",
    "        except Exception as e:\n",
    "            raise\n",
    "        finally:\n",
    "            logging.info(f\"Saving model to {model_path}\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "        return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(\n",
    "#     {\n",
    "#         \"batch_size\": 64,\n",
    "#         \"edit_count\": 25,\n",
    "#         \"bin_count\": 32,\n",
    "#         \"clip_gradients\": True,\n",
    "#         \"learning_rate\": 0.005,\n",
    "#         \"scheduler_gamma\": 0.7,\n",
    "#         \"num_epochs\": 20,\n",
    "#         \"model_type\": \"NormalisedCNN\",\n",
    "#         \"loss\": \"progressive\",\n",
    "#         \"loss_sizes\": [16, 32],\n",
    "#         \"loss_damping\": 2,\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 22:46:07,734 - INFO - Starting run_96 with hparams {\n",
      "  \"batch_size\": 16,\n",
      "  \"bin_count\": 32,\n",
      "  \"clip_gradients\": false,\n",
      "  \"edit_count\": 16,\n",
      "  \"learning_rate\": 3.291322467520231e-05,\n",
      "  \"loss\": \"progressive\",\n",
      "  \"loss_damping\": 1.1967321790868395,\n",
      "  \"loss_sizes\": [\n",
      "    8,\n",
      "    32\n",
      "  ],\n",
      "  \"model_type\": \"attention2\",\n",
      "  \"num_epochs\": 5,\n",
      "  \"scheduler_gamma\": 0.4573812925553331\n",
      "}\n",
      "2024-06-03 22:46:07,762 - INFO - Loaded 359668 training images and 39964 test images\n",
      "2024-06-04 00:08:08,259 - INFO - Epoch 0 loss: 38.42179161275271\n",
      "/home/andras/miniconda3/envs/bipolaroid/lib/python3.12/site-packages/matplotlib/collections.py:996: RuntimeWarning: invalid value encountered in sqrt\n",
      "  scale = np.sqrt(self._sizes) * dpi / 72.0 * self._factor\n",
      "2024-06-04 01:30:02,938 - INFO - Epoch 1 loss: 34.078268383513205\n",
      "2024-06-04 02:46:08,066 - INFO - Saving model to /home/andras/projects/bipolaroid/models/run_96.pth\n",
      "2024-06-04 02:46:08,182 - WARNING - Timeout, aborting experiment\n",
      "2024-06-04 02:46:08,479 - INFO - Starting run_97 with hparams {\n",
      "  \"batch_size\": 64,\n",
      "  \"bin_count\": 32,\n",
      "  \"clip_gradients\": true,\n",
      "  \"edit_count\": 8,\n",
      "  \"learning_rate\": 5.96886240713341e-05,\n",
      "  \"loss\": \"progressive\",\n",
      "  \"loss_damping\": 2.8893045711729517,\n",
      "  \"loss_sizes\": [\n",
      "    4,\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"model_type\": \"attention2\",\n",
      "  \"num_epochs\": 10,\n",
      "  \"scheduler_gamma\": 0.9315193474157711\n",
      "}\n",
      "2024-06-04 02:46:08,500 - INFO - Loaded 179834 training images and 19982 test images\n",
      "2024-06-04 06:46:16,877 - INFO - Saving model to /home/andras/projects/bipolaroid/models/run_97.pth\n",
      "2024-06-04 06:46:28,422 - WARNING - Timeout, aborting experiment\n",
      "2024-06-04 06:46:28,437 - INFO - Starting run_98 with hparams {\n",
      "  \"batch_size\": 64,\n",
      "  \"bin_count\": 16,\n",
      "  \"clip_gradients\": false,\n",
      "  \"edit_count\": 16,\n",
      "  \"learning_rate\": 0.0019552772361485543,\n",
      "  \"loss\": \"kl\",\n",
      "  \"model_type\": \"SimpleCNN\",\n",
      "  \"num_epochs\": 5,\n",
      "  \"scheduler_gamma\": 0.022346077394851838\n",
      "}\n",
      "2024-06-04 06:46:28,475 - INFO - Loaded 359668 training images and 39964 test images\n",
      "2024-06-04 06:48:11,407 - INFO - Epoch 0 loss: 20430.093976140022\n",
      "2024-06-04 06:49:52,288 - INFO - Epoch 1 loss: 14717.722860097885\n",
      "2024-06-04 06:51:32,993 - INFO - Epoch 2 loss: 13855.800803661346\n",
      "2024-06-04 06:53:13,588 - INFO - Epoch 3 loss: 13853.357687234879\n",
      "2024-06-04 06:54:54,389 - INFO - Epoch 4 loss: 13853.240978479385\n",
      "2024-06-04 06:54:56,519 - INFO - Saving model to /home/andras/projects/bipolaroid/models/run_98.pth\n",
      "2024-06-04 06:54:57,057 - INFO - Starting run_99 with hparams {\n",
      "  \"batch_size\": 32,\n",
      "  \"bin_count\": 32,\n",
      "  \"clip_gradients\": false,\n",
      "  \"edit_count\": 16,\n",
      "  \"learning_rate\": 0.00041782149104212284,\n",
      "  \"loss\": \"progressive\",\n",
      "  \"loss_damping\": 2.393572363792762,\n",
      "  \"loss_sizes\": [\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"model_type\": \"attention2\",\n",
      "  \"num_epochs\": 10,\n",
      "  \"scheduler_gamma\": 0.3478968531660309\n",
      "}\n",
      "2024-06-04 06:54:57,082 - INFO - Loaded 359668 training images and 39964 test images\n",
      "2024-06-04 07:28:59,180 - INFO - Saving model to /home/andras/projects/bipolaroid/models/run_99.pth\n",
      "2024-06-04 07:28:59,341 - INFO - Interrupted, stopping\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "from itertools import count\n",
    "import json\n",
    "\n",
    "\n",
    "for _ in count():\n",
    "    current_hyperparameters = {\n",
    "        k: v.rvs() if hasattr(v, \"rvs\") else choice(v)\n",
    "        for k, v in choice(hyperparameters).items()\n",
    "    }\n",
    "    key = json.dumps(current_hyperparameters, indent=2, sort_keys=True)\n",
    "    logging.info(\n",
    "        f\"Starting {get_next_run_name(Path(\"runs\"))} with hparams {key}\"\n",
    "    )\n",
    "    try:\n",
    "        train(current_hyperparameters, max_duration=timedelta(hours=4), use_tqdm=False)\n",
    "    except KeyboardInterrupt as e:\n",
    "        logging.info(\"Interrupted, stopping\")\n",
    "        break\n",
    "    except TimeoutError as e:\n",
    "        logging.warning(f\"Timeout, aborting experiment\")\n",
    "    except Exception as e:\n",
    "        logging.error(\n",
    "            f\"Error with hparams {current_hyperparameters}:\\n\\t{e}\", stack_info=True\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bipolaroid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
