{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Using device cuda:0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "f\"Using device {device}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_hyperparameters = {\n",
    "    \"batch_size\": [8, 16, 32, 64],\n",
    "    \"edit_count\": [4, 8, 16, 32],\n",
    "    \"bin_count\": [32],\n",
    "    \"clip_gradients\": [True, False],\n",
    "    \"learning_rate\": [0.0001],\n",
    "    \"scheduler_gamma\": [0.1, 0.9],\n",
    "    \"num_epochs\": [20],\n",
    "    \"model_type\": [\"v1\"],\n",
    "}\n",
    "hyperparameters = [\n",
    "    {\n",
    "        **common_hyperparameters,\n",
    "        \"loss\": [\"progressive\"],\n",
    "        \"loss_sizes\": [[4, 8, 16, 32], [8, 16, 32], [16, 32], [8, 32]],\n",
    "        \"loss_damping\": [1, 2, 3, 4, 5],\n",
    "    },\n",
    "    {\n",
    "        **common_hyperparameters,\n",
    "        \"loss\": [\"kl\"],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "from torch.optim import Adam\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from editor.training import ProgressivePoolingLoss\n",
    "from editor.utils import get_next_run_name\n",
    "from editor.visualisation import plot_histograms_in_2d\n",
    "from editor.training import create_data_loaders\n",
    "from editor.models import create_model\n",
    "from config import DATA, MODELS_PATH\n",
    "\n",
    "\n",
    "def train(hyperparameters: Dict[str, Any]) -> Path:\n",
    "    model_path = (MODELS_PATH / get_next_run_name(Path(\"runs\"))).with_suffix(\"pth\")\n",
    "\n",
    "    log_dir = Path(\"runs\") / get_next_run_name(Path(\"runs\"))\n",
    "    with SummaryWriter(log_dir) as writer:\n",
    "        train_data_loader, test_data_loader = create_data_loaders(\n",
    "            data=DATA,\n",
    "            edit_count=hyperparameters[\"edit_count\"],\n",
    "            bin_count=hyperparameters[\"bin_count\"],\n",
    "            training_batch_size=hyperparameters[\"batch_size\"],\n",
    "        )\n",
    "\n",
    "        model = (\n",
    "            create_model(\n",
    "                type=hyperparameters[\"model_type\"],\n",
    "                bin_count=hyperparameters[\"bin_count\"],\n",
    "            )\n",
    "            .train()\n",
    "            .to(device)\n",
    "        )\n",
    "        writer.add_graph(model, next(iter(train_data_loader))[0].to(device))\n",
    "\n",
    "        optimizer = Adam(model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer, step_size=1, gamma=hyperparameters[\"scheduler_gamma\"]\n",
    "        )\n",
    "\n",
    "        loss_function = {\n",
    "            \"progressive\": lambda: ProgressivePoolingLoss(\n",
    "                target_sizes=hyperparameters[\"loss_sizes\"],\n",
    "                damping=hyperparameters[\"loss_damping\"],\n",
    "            ),\n",
    "            \"kl\": lambda: torch.nn.KLDivLoss(reduction=\"batchmean\"),\n",
    "        }[hyperparameters[\"loss\"]]().to(device)\n",
    "\n",
    "        try:\n",
    "            for epoch in range(hyperparameters[\"num_epochs\"]):\n",
    "                epoch_loss = 0\n",
    "                writer.add_scalar(\n",
    "                    \"Actual learning rate\", scheduler.get_last_lr()[0], epoch\n",
    "                )\n",
    "                for batch_id, (edited_histogram, original_histogram) in enumerate(\n",
    "                    tqdm(train_data_loader, desc=f\"Epoch {epoch}\", unit=\"batch\")\n",
    "                ):\n",
    "                    edited_histogram = edited_histogram.to(device)\n",
    "                    original_histogram = original_histogram.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    predicted_original = model(edited_histogram)\n",
    "\n",
    "                    if hyperparameters[\"loss\"] == \"kl\":\n",
    "                        predicted_original = torch.clamp(\n",
    "                            predicted_original, 0.0000000000000000000000001, 1\n",
    "                        )\n",
    "\n",
    "                    loss = {\n",
    "                        \"kl\": lambda: loss_function(\n",
    "                            torch.log(predicted_original.unsqueeze(1)),\n",
    "                            original_histogram,\n",
    "                        ),\n",
    "                        \"progressive\": lambda: loss_function(\n",
    "                            predicted_original.unsqueeze(1), original_histogram\n",
    "                        ),\n",
    "                    }[hyperparameters[\"loss\"]]()\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "                    writer.add_scalar(\n",
    "                        \"Loss/train/batch\",\n",
    "                        loss,\n",
    "                        epoch * len(train_data_loader) + batch_id,\n",
    "                    )\n",
    "                    loss.backward()\n",
    "\n",
    "                    if hyperparameters[\"clip_gradients\"]:\n",
    "                        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "\n",
    "                writer.add_hparams(\n",
    "                    {\n",
    "                        k: str(v) if isinstance(v, list) else v\n",
    "                        for k, v in hyperparameters.items()\n",
    "                    },\n",
    "                    {\n",
    "                        \"Loss/train/epoch\": epoch_loss,\n",
    "                    },\n",
    "                    global_step=epoch,\n",
    "                    run_name=log_dir.absolute(),\n",
    "                )\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    loader = iter(test_data_loader)\n",
    "                    edited_histogram, original_histogram = next(loader)\n",
    "                    edited_histogram = edited_histogram.to(device)\n",
    "                    original_histogram = original_histogram.to(device)\n",
    "                    predicted_original = model(edited_histogram)\n",
    "                    writer.add_figure(\n",
    "                        \"histogram\",\n",
    "                        plot_histograms_in_2d(\n",
    "                            {\n",
    "                                \"original\": original_histogram.cpu()[0]\n",
    "                                .numpy()\n",
    "                                .squeeze(),\n",
    "                                \"edited\": edited_histogram.cpu()[0].numpy().squeeze(),\n",
    "                                \"predicted\": predicted_original.cpu()[0]\n",
    "                                .numpy()\n",
    "                                .squeeze(),\n",
    "                            }\n",
    "                        ),\n",
    "                        epoch,\n",
    "                    )\n",
    "                    model.train()\n",
    "                scheduler.step()\n",
    "        finally:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 19:44:36,319 - INFO - Loaded 561982 training images and 62443 test images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866f474ead21440e81dc91f2d0e55046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/17562 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andras/miniconda3/envs/bipolaroid/lib/python3.12/site-packages/matplotlib/collections.py:996: RuntimeWarning: invalid value encountered in sqrt\n",
      "  scale = np.sqrt(self._sizes) * dpi / 72.0 * self._factor\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e35a195b784da98ff4b21e493420c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/17562 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(\n",
    "    {\n",
    "        \"batch_size\": 32,\n",
    "        \"edit_count\": 25,\n",
    "        \"bin_count\": 32,\n",
    "        \"clip_gradients\": True,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"scheduler_gamma\": 0.5,\n",
    "        \"num_epochs\": 20,\n",
    "        \"model_type\": \"v1\",\n",
    "        \"loss\": \"kl\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loaded 28800 training images and 3200 test images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run 0: {'batch_size': 32, 'edit_count': 32, 'bin_count': 32, 'clip_gradients': False, 'learning_rate': 0.0001, 'scheduler_gamma': 0.9, 'num_epochs': 20, 'model_type': 'v1', 'loss': 'progressive', 'loss_sizes': [16, 32], 'loss_damping': 5}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f937ddaa2694b0db368c7a9bdd11330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/900 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "value should be one of int, float, str, bool, or torch.Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 99\u001b[0m\n\u001b[1;32m     96\u001b[0m         clip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     97\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 99\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_hparams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurrent_hyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoss/train/epoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabsolute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    108\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/envs/bipolaroid/lib/python3.12/site-packages/torch/utils/tensorboard/writer.py:341\u001b[0m, in \u001b[0;36mSummaryWriter.add_hparams\u001b[0;34m(self, hparam_dict, metric_dict, hparam_domain_discrete, run_name, global_step)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(hparam_dict) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mdict\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(metric_dict) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhparam_dict and metric_dict should be dictionary.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 341\u001b[0m exp, ssi, sei \u001b[38;5;241m=\u001b[39m \u001b[43mhparams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparam_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparam_domain_discrete\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m run_name:\n\u001b[1;32m    344\u001b[0m     run_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime())\n",
      "File \u001b[0;32m~/miniconda3/envs/bipolaroid/lib/python3.12/site-packages/torch/utils/tensorboard/summary.py:316\u001b[0m, in \u001b[0;36mhparams\u001b[0;34m(hparam_dict, metric_dict, hparam_domain_discrete)\u001b[0m\n\u001b[1;32m    314\u001b[0m         hps\u001b[38;5;241m.\u001b[39mappend(HParamInfo(name\u001b[38;5;241m=\u001b[39mk, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mDataType\u001b[38;5;241m.\u001b[39mValue(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATA_TYPE_FLOAT64\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue should be one of int, float, str, bool, or torch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    318\u001b[0m     )\n\u001b[1;32m    320\u001b[0m content \u001b[38;5;241m=\u001b[39m HParamsPluginData(session_start_info\u001b[38;5;241m=\u001b[39mssi, version\u001b[38;5;241m=\u001b[39mPLUGIN_DATA_VERSION)\n\u001b[1;32m    321\u001b[0m smd \u001b[38;5;241m=\u001b[39m SummaryMetadata(\n\u001b[1;32m    322\u001b[0m     plugin_data\u001b[38;5;241m=\u001b[39mSummaryMetadata\u001b[38;5;241m.\u001b[39mPluginData(\n\u001b[1;32m    323\u001b[0m         plugin_name\u001b[38;5;241m=\u001b[39mPLUGIN_NAME, content\u001b[38;5;241m=\u001b[39mcontent\u001b[38;5;241m.\u001b[39mSerializeToString()\n\u001b[1;32m    324\u001b[0m     )\n\u001b[1;32m    325\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: value should be one of int, float, str, bool, or torch.Tensor"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "for i in count():\n",
    "    current_hyperparameters = {k: choice(v) for k, v in choice(hyperparameters).items()}\n",
    "    logging.info(f\"Starting run {i} with hparams {current_hyperparameters}\")\n",
    "    try:\n",
    "        train(current_hyperparameters)\n",
    "    except KeyboardInterrupt as e:\n",
    "        logging.info(\"Interrupted, stopping\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error with hparams {current_hyperparameters}:\\n\\t{e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: try different colour spaces, see the results applied to images\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bipolaroid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
